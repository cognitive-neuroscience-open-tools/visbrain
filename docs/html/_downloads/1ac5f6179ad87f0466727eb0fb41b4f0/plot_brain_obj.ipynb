{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Brain object (BrainObj) : complete tutorial\n\nThis example illustrate the main functionalities and inputs of the brain\nobject i.e :\n\n  * Use included MNI brain template\n  * Select the hemisphere ('both', 'left', 'right')\n  * Use a translucent or opaque brain\n  * Project source's activity on the surface of the brain\n  * Parcellize the brain and send data to selected parcellates\n  * Add fMRI activation and MEG inverse solution\n\nData for fMRI activations and MEG inverse solutoin comes from the PySurfer\nsoftware (https://github.com/nipy/PySurfer/). Parcellation file comes from\nMNE-Python (https://github.com/mne-tools/mne-python).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom visbrain.objects import BrainObj, ColorbarObj, SceneObj, SourceObj\nfrom visbrain.io import download_file, read_stc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scene creation\n The SceneObj is Matplotlib subplot like in which, you can add visbrain's\n objects. We first create the scene with a black background, a fixed size\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Scene creation\nsc = SceneObj(bgcolor='black', size=(1400, 1000))\n# Colorbar default arguments. See `visbrain.objects.ColorbarObj`\nCBAR_STATE = dict(cbtxtsz=12, txtsz=10., width=.1, cbtxtsh=3.,\n                  rect=(-.3, -2., 1., 4.))\nKW = dict(title_size=14., zoom=1.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The BrainObj can interact with sources (SourceObj). For example, if the\n    source object represent intracranial data (e.g iEEG) those sources can\n    be projected on the surface of the brain. This is an important feature\n    because intracranial implantations is usually subject dependant and the\n    projection is a good way to plot results across subjects. To illustrate\n    this feature, we provide a set of intracranial MNI coordinates.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download iEEG coordinates and define some random data\nmat = np.load(download_file('xyz_sample.npz', astype='example_data'))\nxyz, subjects = mat['xyz'], mat['subjects']\ndata = np.random.rand(xyz.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic brain using MNI template\n By default, Visbrain include several MNI brain templates (B1, B3, B3,\n inflated, white and shere).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Translucent inflated BrainObj with both hemispheres displayed\nb_obj_fs = BrainObj('inflated', translucent=True, hemisphere='both')\n# Add the brain to the scene. Note that `row_span` means that the plot will\n# occupy two rows (row 0 and 1)\nsc.add_to_subplot(b_obj_fs, row=0, col=0, row_span=2,\n                  title='Translucent inflated brain template', **KW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select the left or the right hemisphere\n You can use the `hemisphere` input to select either the 'left', 'right' or\n 'both' hemispheres.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Opaque left hemispehre of the white matter\nb_obj_lw = BrainObj('white', hemisphere='left', translucent=False)\nsc.add_to_subplot(b_obj_lw, row=0, col=1, rotate='right',\n                  title='Left hemisphere', **KW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projection iEEG data on the surface of the brain\n As explain above, we define a source object and project the source's activity\n on the surface of the brain\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First, define a brain object used for the projection\nb_obj_proj = BrainObj('B3', hemisphere='both', translucent=False)\n# Define the source object\ns_obj = SourceObj('iEEG', xyz, data=data, cmap='inferno')\n# Just for fun, color sources according to the data :)\ns_obj.color_sources(data=data)\n# Project source's activity\ns_obj.project_sources(b_obj_proj, cmap='plasma')\n# Finally, add the source and brain objects to the subplot\nsc.add_to_subplot(s_obj, row=0, col=2, title='Project iEEG data', **KW)\nsc.add_to_subplot(b_obj_proj, row=0, col=2, rotate='left', use_this_cam=True)\n# Finally, add the colorbar :\ncb_proj = ColorbarObj(s_obj, cblabel='Projection of niEEG data', **CBAR_STATE)\nsc.add_to_subplot(cb_proj, row=0, col=3, width_max=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Here, we used s_obj.project_sources(b_obj) to project source's activity\n    on the surface. We could also have used to b_obj.project_sources(s_obj)</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parcellize the brain\n Here, we parcellize the brain (using all parcellated included in the file).\n Note that those parcellates files comes from MNE-python.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download the annotation file of the left hemisphere lh.aparc.a2009s.annot\npath_to_file1 = download_file('lh.aparc.a2009s.annot', astype='example_data')\n# Define the brain object (now you should know how to do it)\nb_obj_parl = BrainObj('inflated', hemisphere='left', translucent=False)\n# Print parcellates included in the file\n# print(b_obj_parl.get_parcellates(path_to_file1))\n# Finally, parcellize the brain and add the brain to the scene\nb_obj_parl.parcellize(path_to_file1)\nsc.add_to_subplot(b_obj_parl, row=1, col=1, rotate='left',\n                  title='Parcellize using the Desikan Atlas', **KW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Those annotations files from MNE-python are only compatibles with the\n    inflated, white and sphere templates</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Send data to parcellates\n Again, we download an annotation file, but this time for the right hemisphere\n The difference with the example above, is that this time we send some data\n to some specific parcellates\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download the annotation file of the right hemisphere rh.aparc.annot\npath_to_file2 = download_file('rh.aparc.annot', astype='example_data')\n# Define the brain object (again... I know, this is redundant)\nb_obj_parr = BrainObj('inflated', hemisphere='right', translucent=False)\n# Print parcellates included in the file\n# print(b_obj_parr.get_parcellates(path_to_file2))\n# From the list of printed parcellates, we only select a few of them\nselect_par = ['paracentral', 'precentral', 'fusiform', 'postcentral',\n              'superiorparietal', 'superiortemporal', 'inferiorparietal',\n              'inferiortemporal']\n# Now we define some data for each parcellates (one value per pacellate)\ndata_par = [10., .1, 5., 7., 11., 8., 4., 6.]\n# Parcellize the brain with the selected parcellates. The data range is\n# between [.1, 11.]. Then, we use `vmin` and `vmax` to specify that we want\n# every parcellates under vmin to be gray and every parcellates over vmax\n# darkred\nb_obj_parr.parcellize(path_to_file2, select=select_par, hemisphere='right',\n                      cmap='viridis', data=data_par, clim=[.1, 11.], vmin=1.,\n                      vmax=10, under='gray', over='darkred')\n# Add the brain object to the scene\nsc.add_to_subplot(b_obj_parr, row=1, col=2, rotate='right',\n                  title='Send data to Desikan-Killiany parcellates', **KW)\n# Get the colorbar of the brain object and add it to the scene\ncb_parr = ColorbarObj(b_obj_parr, cblabel='Data to parcellates', **CBAR_STATE)\nsc.add_to_subplot(cb_parr, row=1, col=3, width_max=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom brain template\n All of the examples above use MNI brain templates that are included inside\n visbrain. But you can define your own brain template using vertices and faces\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download the vertices, faces and normals\nmat = np.load(download_file('Custom.npz', astype='example_data'))\nvert, faces, norms = mat['vertices'], mat['faces'], mat['normals']\n# By default, vertices are in millimeters so we multiply by 1000.\nvert *= 1000.\n# If your template represent a brain with both hemispheres, you can use the\n# `lr_index` to specify which vertices belong to the left or the right\n# hemisphere. Basically, `lr_index` is a boolean vector of shape (n_vertices,)\n# where True reflect locatino of the left hemisphere and False, the right\n# hemisphere\nlr_index = vert[0, :] <= 0.\n# Create the brain object and add it to the scene (this time it's a bit\n# different)\nb_obj_custom = BrainObj('Custom', vertices=vert, faces=faces,\n                        normals=norms, translucent=False)\nsc.add_to_subplot(b_obj_custom, row=2, col=0, title='Use a custom template',\n                  rotate='left', **KW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you doesn't have the normals, it's not a big deal because if no\n    normals are provided, normals are going to be computed but it's a bit\n    slower. Then, you can save your template using `BrainObj.save`. This can\n    be convenient to reload your template later.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## fMRI activation\n Add fMRI activations (included in a nii.gz file) to the surface. The provided\n file comes from MNE-python\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download the lh.sig.nii.gz file\nfile = download_file('lh.sig.nii.gz', astype='example_data')\n# Define the [...] you know\nb_obj_fmri = BrainObj('inflated', translucent=False, sulcus=True)\n# Add fMRI activation and hide every activation that is under 5.\nb_obj_fmri.add_activation(file=file, clim=(5., 20.), hide_under=5,\n                          cmap='viridis', hemisphere='left')\nsc.add_to_subplot(b_obj_fmri, row=2, col=1, title='Add fMRI activation',\n                  rotate='left', **KW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MEG inverse solution\n Finally, plot MEG inverse solution. The provided file comes from MNE-python\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Dowload meg_source_estimate-rh.stc file and load the data\nfile = read_stc(download_file('meg_source_estimate-rh.stc',\n                              astype='example_data'))\n# Get the data of index 2 and the vertices\ndata = file['data'][:, 2]\nvertices = file['vertices']\n# You know...\nb_obj_meg = BrainObj('inflated', translucent=False, hemisphere='right',\n                     sulcus=True)\n# Add MEG data to the surface and hide every values under 5.\nb_obj_meg.add_activation(data=data, vertices=vertices, hemisphere='right',\n                         smoothing_steps=21, clim=(5., 17.), hide_under=5.,\n                         cmap='plasma')\n# Add the brain and the colorbar object to the scene\nsc.add_to_subplot(b_obj_meg, row=2, col=2, title='MEG inverse solution',\n                  rotate='right', **KW)\ncb_parr = ColorbarObj(b_obj_meg, cblabel='MEG data', **CBAR_STATE)\nsc.add_to_subplot(cb_parr, row=2, col=3, width_max=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \"Fun\" stuff\n You can link 3D rotations of subplots which means that if you rotate one\n brain, the other linked object inherit from the same rotations. Finally, you\n can take a screenshot of the scene, without the need to open the window.\n This can be particulary convenient when scenes are included inside loops to\n automatize figure generation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Link the rotation of subplots (row=0, col=1) and (row=1, col=2)\n# sc.link((0, 1), (1, 2))\n# Screenshot of the scene\n# sc.screenshot('ex_brain_obj.png', transparent=True)\n\nsc.preview()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}